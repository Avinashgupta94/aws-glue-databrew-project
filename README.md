# AWS Glue DataBrew Project

This project demonstrates end-to-end data preparation, cleaning, and transformation using **AWS Glue DataBrew** with Amazon S3 datasets (`customers.csv` and `sales.csv`).  
The goal is to showcase practical **ETL (Extract, Transform, Load)** skills that align with **Data Engineer I** requirements at Amazon and other top companies.

---

## ğŸš€ Project Overview
- Built a **DataBrew project** to clean and transform raw customer and sales data stored in **Amazon S3**.  
- Created **recipes** with multiple transformation steps for data cleaning, formatting, and enrichment.  
- Generated **data lineage** to track the flow from raw data â†’ transformed dataset â†’ output in JSON/Parquet.  
- Ensured data quality using **DataBrew profiling** (column statistics, missing values, uniqueness, PII checks).  
- Created an **ETL job in AWS Glue Studio** to combine Customers and Sales datasets and write processed data back to S3.

---

## ğŸ“‚ Project Structure
```
aws-glue-databrew-project/
â”œâ”€â”€ data/                      # optional sample CSVs (if included)
â”œâ”€â”€ docs/                      # optional extra documentation
â”œâ”€â”€ screenshots/               # screenshots proving work (see list below)
â””â”€â”€ README.md                  # this file
```

---

## ğŸ—‚ Datasets
- **Customers dataset** â€” customer details (Prefix, First_Name, Last_Name, Gender, DoB, Address, Zip, City, Country)  
- **Sales dataset** â€” transaction records (Txn_Date, Customer_Id, Product_Id, Quantity, Total_Sales)  

> Both datasets are AWS sample CSVs used for learning and demonstration.

---

## ğŸ›  Tools & Services
- **AWS Glue DataBrew** â€” data profiling, interactive recipe authoring, cleaning  
- **AWS Glue Studio** â€” visual ETL job for joins, aggregates, and final output  
- **Amazon S3** â€” raw and processed data storage  
- **AWS Glue Data Catalog** â€” metadata & schema (optional)

---

## ğŸ”§ Key Transformations (CleanCustomer recipe)
1. Remove unnecessary columns (`Middle_Name`, `Suffix`).  
2. Reformat `DoB` to `MM-dd-yyyy` and mask later for privacy.  
3. Remove special characters from `Address`.  
4. Split `Address` into address parts using delimiter (`;`).  
5. Rename split columns:  
   - `Address_3` â†’ `City`  
   - `Address_4` â†’ `Zip`  
   - `Address_5` â†’ `Country`  
6. Merge `Prefix + First_Name + Last_Name` â†’ `Name` (or `Full_Name`).  
7. Convert `Name` to UPPERCASE.  
8. Mask `DoB` values (e.g., `1970-01-01` â†’ `####-##-##`) for privacy in outputs (if required).

---

## ğŸ”— Data Lineage & Quality
- **Lineage:** Raw CSV (S3) â†’ DataBrew recipe (transformations) â†’ DataBrew output / Glue Studio job â†’ Processed files (S3 JSON/Parquet).  
- **Quality checks:** Column statistics, uniqueness, missing-value reports, simple PII identification were run in DataBrew.

---

## ğŸ“¸ Required screenshots (place in `screenshots/` folder)
Put these images into `screenshots/` with exactly these filenames so image links render correctly on GitHub:

- `customers_dataset_preview.png`  
- `sales_dataset_preview.png`  
- `customers_column_statistics.png`  
- `clean_customer_project_creation.png`  
- `clean_customer_provisioning.png`  
- `clean_customer_recipe_steps.png`  
- `clean_customer_profiled_view.png`  
- `customers_address_city_country.png`  
- `customers_data_lineage.png`

(If some screenshots donâ€™t exist, include the ones you have and remove unused names.)

---

## ğŸ¯ Learning Outcomes
- Hands-on experience with DataBrew recipes, profiling, and Glue Studio.  
- Practical understanding of designing a production-like ETL pipeline.  
- Portfolio-ready demonstration of AWS data-workflow skills for interviews.

---

## ğŸ¤ Notes
- Datasets used are **AWS sample data** and contain no real customer info.  
- This project is for learning and portfolio purposes only.
